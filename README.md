# VideoScore
This is the official repo for our EMNLP 2024 paper "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation".

<a target="_blank" href="https://arxiv.org/abs/2406.15252">
<img style="height:22pt" src="https://img.shields.io/badge/-Paper-red?style=flat&logo=arxiv"></a>
<a target="_blank" href="https://github.com/TIGER-AI-Lab/VideoScore">
<img style="height:22pt" src="https://img.shields.io/badge/-Code-green?style=flat&logo=github"></a>
<a target="_blank" href="https://tiger-ai-lab.github.io/VideoScore/">
<img style="height:22pt" src="https://img.shields.io/badge/-ğŸŒ%20Website-blue?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/TIGER-Lab/VideoFeedback">
<img style="height:22pt" src="https://img.shields.io/badge/-ğŸ¤—%20Dataset-red?style=flat"></a>
<a target="_blank" href="https://huggingface.co/spaces/TIGER-Lab/VideoScore">
<img style="height:22pt" src="https://img.shields.io/badge/-ğŸ¤—%20Demo-red?style=flat"></a> 
<a target="_blank" href="https://huggingface.co/TIGER-Lab/VideoScore">
<img style="height:22pt" src="https://img.shields.io/badge/-ğŸ¤—%20Models-red?style=flat"></a>
<a target="_blank" href="https://twitter.com/DongfuJiang/status/1805438506137010326">
<img style="height:22pt" src="https://img.shields.io/badge/-Tweet-blue?style=flat&logo=twitter"></a>
<br>

# VideoScore

## æ¦‚è¿°
**VideoScore** é¡¹ç›®æ—¨åœ¨ä»å¤šä¸ªç»´åº¦è¯„ä¼°è§†é¢‘è´¨é‡ã€‚å®ƒæä¾›äº†å¯¹è§†è§‰è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§ã€åŠ¨æ€ç¨‹åº¦ã€æ–‡æœ¬ä¸è§†é¢‘å¯¹é½ä»¥åŠäº‹å®ä¸€è‡´æ€§çš„å…¨é¢è¯„ä¼°ã€‚è¯¥é¡¹ç›®ä½¿ç”¨ Python æ„å»ºï¼Œå¹¶åˆ©ç”¨å„ç§åº“æ¥åˆ†æå’Œè¯„åˆ†è§†é¢‘å†…å®¹ã€‚

## å…¥é—¨æŒ‡å—

### å‰ææ¡ä»¶
åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…ä»¥ä¸‹è½¯ä»¶ï¼š
- Python 3.x
- Git

### å®‰è£…
1. å…‹éš†ä»“åº“ï¼š
   ```bash
   git clone https://huggingface.co/spaces/svjack/VideoScore && cd VideoScore
   ```

2. å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š
   ```bash
   pip install -r requirements.txt
   ```

## ç»´åº¦æ˜ å°„
ä»¥ä¸‹æ˜¯ `aspect_mapping` ä¸­å„ä¸ªè¯æ±‡çš„æ¶µä¹‰ï¼š

1. **è§†è§‰è´¨é‡**:
   - **æ¶µä¹‰**: è§†è§‰è´¨é‡
   - **è§£é‡Š**: è§†é¢‘åœ¨æ¸…æ™°åº¦ã€åˆ†è¾¨ç‡ã€äº®åº¦å’Œè‰²å½©ç­‰æ–¹é¢çš„è´¨é‡ã€‚è¿™ä¸ªç»´åº¦è¯„ä¼°è§†é¢‘çš„è§†è§‰è¡¨ç°ï¼ŒåŒ…æ‹¬å›¾åƒçš„æ¸…æ™°åº¦ã€è‰²å½©çš„å‡†ç¡®æ€§å’Œæ•´ä½“çš„è§†è§‰å¸å¼•åŠ›ã€‚

2. **æ—¶é—´ä¸€è‡´æ€§**:
   - **æ¶µä¹‰**: æ—¶é—´ä¸€è‡´æ€§
   - **è§£é‡Š**: è§†é¢‘ä¸­ç‰©ä½“æˆ–äººç‰©çš„ä¸€è‡´æ€§ã€‚è¿™ä¸ªç»´åº¦è¯„ä¼°è§†é¢‘åœ¨æ—¶é—´ä¸Šçš„è¿è´¯æ€§ï¼Œå³è§†é¢‘ä¸­çš„ç‰©ä½“æˆ–äººç‰©åœ¨ä¸åŒå¸§ä¹‹é—´æ˜¯å¦ä¿æŒä¸€è‡´ï¼Œæ²¡æœ‰æ˜æ˜¾çš„è·³è·ƒæˆ–ä¸è¿è´¯çš„ç°è±¡ã€‚

3. **åŠ¨æ€ç¨‹åº¦**:
   - **æ¶µä¹‰**: åŠ¨æ€ç¨‹åº¦
   - **è§£é‡Š**: è§†é¢‘ä¸­åŠ¨æ€å˜åŒ–çš„ç¨‹åº¦ã€‚è¿™ä¸ªç»´åº¦è¯„ä¼°è§†é¢‘çš„åŠ¨æ€æ€§ï¼Œå³è§†é¢‘ä¸­ç‰©ä½“æˆ–åœºæ™¯çš„å˜åŒ–ç¨‹åº¦ï¼ŒåŒ…æ‹¬è¿åŠ¨çš„é¢‘ç‡å’Œå¹…åº¦ã€‚

4. **æ–‡æœ¬ä¸è§†é¢‘å¯¹é½**:
   - **æ¶µä¹‰**: æ–‡æœ¬ä¸è§†é¢‘çš„å¯¹é½
   - **è§£é‡Š**: æ–‡æœ¬æç¤ºä¸è§†é¢‘å†…å®¹ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚è¿™ä¸ªç»´åº¦è¯„ä¼°è§†é¢‘å†…å®¹ä¸ç»™å®šæ–‡æœ¬æç¤ºä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ï¼Œå³è§†é¢‘æ˜¯å¦å‡†ç¡®åœ°åæ˜ äº†æ–‡æœ¬æç¤ºæ‰€æè¿°çš„å†…å®¹ã€‚

5. **äº‹å®ä¸€è‡´æ€§**:
   - **æ¶µä¹‰**: äº‹å®ä¸€è‡´æ€§
   - **è§£é‡Š**: è§†é¢‘å†…å®¹ä¸å¸¸è¯†å’Œäº‹å®çŸ¥è¯†çš„ä¸€è‡´æ€§ã€‚è¿™ä¸ªç»´åº¦è¯„ä¼°è§†é¢‘å†…å®¹æ˜¯å¦ç¬¦åˆå¸¸è¯†å’Œäº‹å®çŸ¥è¯†ï¼Œå³è§†é¢‘ä¸­çš„å†…å®¹æ˜¯å¦çœŸå®å¯ä¿¡ï¼Œæ²¡æœ‰æ˜æ˜¾çš„é€»è¾‘é”™è¯¯æˆ–ä¸ç°å®ä¸ç¬¦çš„æƒ…å†µã€‚

è¿™äº›ç»´åº¦å…±åŒæ„æˆäº†å¯¹è§†é¢‘è´¨é‡çš„å¤šæ–¹é¢è¯„ä¼°ï¼Œæ¶µç›–äº†ä»è§†è§‰è¡¨ç°åˆ°å†…å®¹ä¸€è‡´æ€§çš„å„ä¸ªæ–¹é¢ã€‚

## ç¤ºä¾‹
- APP
```bash
python app_regression.py
```
Or
- è„šæœ¬
ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ `video_regression_script.py` çš„ç¤ºä¾‹ï¼š
```bash
python video_regression_script.py "" xiangling_mp4_dir_tiny
```

### ç»“æœåˆ†æ
ä½¿ç”¨ Pandas è¯»å–å¹¶åˆ†æè¯„ä¼°ç»“æœï¼š

```python
import pandas as pd
edf = pd.read_csv("evaluation_results.csv")
edf.describe()
print(edf.sort_values(by = "temporal consistency", ascending = True).head(5).to_markdown())
```

è¾“å‡ºç»“æœï¼š

|    | video_name                                                                                         |   visual quality |   temporal consistency |   dynamic degree |   text-to-video alignment |   factual consistency |
|---:|:---------------------------------------------------------------------------------------------------|-----------------:|-----------------------:|-----------------:|--------------------------:|----------------------:|
| 26 | solo,Xiangling,_shave_with_a_razor__genshin_impact__,1girl,highres,_seed_3140464511.mp4            |             2.8  |                   1.14 |             2.97 |                      2.78 |                  1.26 |
|  0 | solo,Xiangling,_carry_money_in_a_wallet__genshin_impact__,1girl,highres,_seed_1294598571.mp4       |             2.69 |                   1.2  |             2.88 |                      2.7  |                  1.34 |
|  9 | solo,Xiangling,_sweep_dust_with_a_broom__genshin_impact__,1girl,highres,_seed_3483804345.mp4       |             2.72 |                   1.22 |             2.89 |                      2.86 |                  1.2  |
| 25 | solo,Xiangling,_brush_teeth_with_a_toothbrush__genshin_impact__,1girl,highres,_seed_2612536091.mp4 |             2.75 |                   1.23 |             2.91 |                      2.67 |                  1.44 |
| 14 | solo,Xiangling,_store_trash_in_a_bag__genshin_impact__,1girl,highres,_seed_4130052080.mp4          |             2.72 |                   1.25 |             2.86 |                      2.77 |                  1.27 |

## Make video dataset in svjack/Genshin-Impact-XiangLing-animatediff-with-score-organized
```python
import os
import shutil
import uuid
import pandas as pd
from pathlib import Path
from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡

# è¯»å– evaluation_results.csv æ–‡ä»¶
vdf = pd.read_csv("xiangling_benchmark_dir/evaluation_results.csv")

# å°† video_name åˆ—çš„è·¯å¾„è½¬æ¢ä¸ºå®Œæ•´è·¯å¾„
vdf["video_name"] = vdf["video_name"].map(lambda x: os.path.join("xiangling_benchmark_dir/xiangling_mp4_dir_total/", x))

def process_files(input_path, output_path, prefix=""):
    # åˆ›å»ºè¾“å‡ºè·¯å¾„
    os.makedirs(output_path, exist_ok=True)

    # è·å–æ‰€æœ‰ mp4 æ–‡ä»¶
    mp4_files = list(Path(input_path).rglob("*.mp4"))

    # åˆ›å»º metadata åˆ—è¡¨
    metadata = []

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    for mp4_file in tqdm(mp4_files, desc="Processing files"):
        # ç”Ÿæˆ UUID
        unique_id = str(uuid.uuid4())

        # æ„å»ºæ–°çš„æ–‡ä»¶å
        new_mp4_file = Path(output_path) / f"{unique_id}.mp4"
        new_txt_file = Path(output_path) / f"{unique_id}.txt"

        # æ‹·è´ mp4 æ–‡ä»¶åˆ°æ–°è·¯å¾„å¹¶é‡å‘½å
        shutil.copy(mp4_file, new_mp4_file)

        # è§£æè§†é¢‘è·¯å¾„ï¼Œç”Ÿæˆ prompt
        video_name = Path(mp4_file).name  # è·å–æ–‡ä»¶å
        video_name_without_seed = video_name.split("_seed_")[0]  # å»æ‰ _seed_xxxxxx.mp4 éƒ¨åˆ†
        video_name_without_seed = video_name_without_seed.replace("_", " ")  # å°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼

        # åœ¨å†…å®¹å‰æ·»åŠ  prefix
        modified_prompt = f"{prefix}{video_name_without_seed}"

        # å°†ä¿®æ”¹åçš„å†…å®¹å†™å…¥æ–°çš„ txt æ–‡ä»¶
        with open(new_txt_file, "w", encoding="utf-8") as f:
            f.write(modified_prompt)

        # ä» vdf ä¸­æå–ç›¸å…³ä¿¡æ¯
        row = vdf[vdf["video_name"] == str(mp4_file)]
        if not row.empty:
            visual_quality = row["visual quality"].values[0]
            temporal_consistency = row["temporal consistency"].values[0]
            dynamic_degree = row["dynamic degree"].values[0]
            factual_consistency = row["factual consistency"].values[0]

            # æ·»åŠ åˆ° metadata åˆ—è¡¨
            metadata.append({
                "file_name": f"{unique_id}.mp4",
                "prompt": modified_prompt,
                "visual quality": visual_quality,
                "temporal consistency": temporal_consistency,
                "dynamic degree": dynamic_degree,
                "factual consistency": factual_consistency,
                "original_file_name": video_name,  # æ·»åŠ é‡å‘½åå‰çš„æ–‡ä»¶å
            })

    # ç”Ÿæˆ metadata.csv æ–‡ä»¶
    df = pd.DataFrame(metadata)
    df.to_csv(Path(output_path) / "metadata.csv", index=False)

# ç¤ºä¾‹è°ƒç”¨
input_path = "xiangling_benchmark_dir/xiangling_mp4_dir_total"
output_path = "xiangling_benchmark_dir/xiangling_processed"
process_files(input_path, output_path)
```

## And upload by (pip install huggingface_hub)
```bash
huggingface-cli upload svjack/Genshin-Impact-XiangLing-animatediff-with-score-organized xiangling_processed/* . --repo-type dataset
```

## News
[2024-11-28] Try on our new version [VideoScore-v1.1](https://huggingface.co/TIGER-Lab/VideoScore-v1.1), with better performance in **"text-to-video alignment"** subscore and the support for **48 frames** in inference now!

[2024-08-05] We released the Wandb training cruves of [VideoScore](https://api.wandb.ai/links/xuanhe/ptohlfcx) and [VideoScore-anno-only](https://api.wandb.ai/links/xuanhe/4vs5k0cq) to help reproduce the training results.


## Introduction

<video src="https://user-images.githubusercontent.com/105091430/90adfb70-fdff-4101-9207-9bd4f43aae4c.mp4"></video>


ğŸš€The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. 
ğŸ¤”The main barrier is the lack of large-scale human-annotated dataset.

- ğŸ›¢ï¸**VideoFeedback Dataset**. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multiaspect score over 37.6K synthesized videos from 11 existing video generative models.

- ğŸ…**VideoScore**. We train VideoScore (initialized from Mantis) based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman correlation between VideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with human judges than other metrics.

- ğŸ«¡**Human Feedback for Video generative models**. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.

## Installation

- for inference
```bash
pip install -e . 
```
- for evaluation
```bash
pip install -e .[eval] 
```
- for training
```bash
git clone https://github.com/TIGER-AI-Lab/Mantis
cd Mantis
pip install -e .[train,eval]
pip install flash-attn --no-build-isolation
# then training scripts are in Mantis/train/scripts
```

## Dataset
- [ğŸ¤— VideoFeedback](https://huggingface.co/datasets/TIGER-Lab/VideoFeedback) VideoFeedback contains a total of 37.6K text-to-video pairs from 11 popular video generative models, with some real-world videos as data augmentation. The videos are annotated by raters for five evaluation dimensions: Visual Quality, Temporal Consistency, Dynamic Degree, Text-to-Video Alignment and Factual Consistency, in 1-4 scoring scale. 

- [ğŸ¤— VideoScore-Bench](https://huggingface.co/datasets/TIGER-Lab/VideoScore-Bench) 
We derive four test sets from 
[VideoFeedback](https://huggingface.co/datasets/TIGER-Lab/VideoFeedback), 
[EvalCrafter](https://github.com/evalcrafter/EvalCrafter), 
[GenAI-Bench](https://huggingface.co/datasets/TIGER-Lab/GenAI-Bench) and 
[VBench](https://github.com/Vchitect/VBench) respectively to curate VideoScore-Bench. 
VideoScore-Bench is composed of about 7,000 videos, covering both Likert-scale annotation and human preference data.  

## Model
- [ğŸ¤— VideoScore](https://huggingface.co/TIGER-Lab/VideoScore) is a video quality evaluation model, taking [Mantis-8B-Idefics2](https://huggingface.co/TIGER-Lab/Mantis-8B-Idefics2) as base-model and trained on [VideoFeedback](https://huggingface.co/datasets/TIGER-Lab/VideoFeedback). 

- [ğŸ¤— VideoScore-anno-only](https://huggingface.co/TIGER-Lab/VideoScore-anno-only) is a variant of VideoScore, trained on VideoFeedback with the real videos excluded.


## Inference examples
```bash
cd examples
python run_videoscore.py
```

## Evaluation
For details, please check [benchmark/README.md](benchmark/README.md)

## Training
For details, please check [training/README.md](training/README.md)

## Acknowledgement
- Thanks [Mantis](https://github.com/TIGER-AI-Lab/Mantis/tree/main) for the training codebase of VideoScore (and variants) and also for the plug-and-play MLLM tools in evaluation stage! 

- Thanks [VIEScore](https://github.com/TIGER-AI-Lab/VIEScore/tree/main) for some codes of prompting MLLM in evaluation! 

## Citation
```bibtex
@article{he2024videoscore,
  title = {VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation},
  author = {He, Xuan and Jiang, Dongfu and Zhang, Ge and Ku, Max and Soni, Achint and Siu, Sherman and Chen, Haonan and Chandra, Abhranil and Jiang, Ziyan and Arulraj, Aaran and Wang, Kai and Do, Quy Duc and Ni, Yuansheng and Lyu, Bohan and Narsupalli, Yaswanth and Fan, Rongqi and Lyu, Zhiheng and Lin, Yuchen and Chen, Wenhu},
  journal = {ArXiv},
  year = {2024},
  volume={abs/2406.15252},
  url = {https://arxiv.org/abs/2406.15252},
}

```
